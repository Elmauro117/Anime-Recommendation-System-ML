# -*- coding: utf-8 -*-
"""Anime Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1byTdp3uTqUbhAirAUviPv34D7M_j-rOH

### CONTENT BASED
"""

import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import naive_bayes
from sklearn.metrics import roc_auc_score, accuracy_score
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import difflib
from sklearn.metrics.pairwise import cosine_similarity

#df3 = pd.read_csv("AnimeDB.csv")   ## This one has the UNKNOWN values and many UNKNOWN converted randomly as names from Genres, Themes.
#df3 = pd.read_csv("AnimeDB2.csv")    ## This one changes UNKNOWN by "" and Drops rows containing Theme, Genre & Genre2 with "" values
df3 = pd.read_csv("AnimeDB4.csv")

# df3 = df3.drop(["Unnamed: 0","ID"], axis=1)
df3 = df3.drop(["Unnamed: 0"], axis=1)

from sklearn.utils import shuffle
df3=shuffle(df3)

df3.shape

# L are the features we are going to use
# L = ["Genre","Genre2","Theme","Rating","Title","Source","Synopsis"]

L = ["Genres","Themes","Rating","Title","Source","Synopsis"]

for x in L:
  print(df3[x].isna().sum())

df3 = df3.dropna(subset=["Title","Synopsis"])

len(df3["Title"])

#df3['Genre'] = df3['Genre'].str.replace('Unknown', '', regex=True).str.strip()

df3['Genres'] = df3['Genres'].str.replace('Unknown', '', regex=True).str.strip()

#df3['Genre2'] = df3['Genre2'].str.replace('Unknown', '', regex=True).str.strip()
#df3['Theme'] = df3['Theme'].str.replace('ZZZZ', '', regex=True).str.strip()

df3['Themes'] = df3['Themes'].str.replace('ZZZZ', '', regex=True).str.strip()

df3['Rating'] = df3['Rating'].str.replace('Unknown', '', regex=True).str.strip()
# df3['Synopsis'] = df3['Synopsis'].str.replace('unknown', '', regex=True).str.strip() 

df3['Synopsis'] = df3['Synopsis'].str.replace('Unknown', '', regex=True).str.strip()

print(df3.loc[df3.Genres == "Unknown", "Genres"].value_counts()) 
print("------------------------")
print(df3.loc[df3.Title == "Unknown", "Title"].value_counts())
print("------------------------")
print(df3.loc[df3.Themes == "Unknown", "Themes"].value_counts())
print("------------------------")
print(df3.loc[df3.Rating == "Unknown", "Rating"].value_counts())
print("------------------------")
print(df3.loc[df3.Title == "Unknown", "Title"].value_counts())
print("------------------------")
print(df3.loc[df3.Synopsis == "Unknown", "Synopsis"].value_counts())

## Creating a new Index in order
lista = list(range(0,len(df3["Title"])))
len(lista)

df3["ID"] = lista

df3.head(3)

#L = ["Genre","Genre2","Theme","Rating","Title","Source","Synopsis"]
L = ["Genres","Themes","Rating","Title","Source","Synopsis"]

for x in L:
  df3[x] = df3[x].str.lower()

# combined_features = df3['Genre']+' '+df3['Genre2']+' '+df3['Theme']+' '+df3['Studio']+' '+df3['Rating']+' '+df3['Source']+' '+df3['Title']

#combined_features = df3['Genre']+' '+df3['Genre2']+' '+df3['Theme'] #+  ' '+df3['Rating'] #+' '+df3['Synopsis']  ## +' '+df3['Source']

# combined_features = df3['Synopsis'] #+' '+df3['Rating']


combined_features = df3['Genres']+' '+df3['Themes']  +' '+df3['Rating'] # +' '+df3['Rating'] 

## Among many combinations it works better the second one, Synops and Rating have little predictive power, so it's safe to throw them.
##  Source is pretty useless too. BUT some animes on the dataset don't have Genre &/or Genre2-Theme, so Rating Synop and Source can help predict

"""Rating and Synopsis improves little to nothing """

vectorizer = TfidfVectorizer()

feature_vectors = vectorizer.fit_transform(combined_features)

# We use Cosine Similarity 
## This is the ALGO

similarity = cosine_similarity(feature_vectors)

#  list_titles = df3['Genre']+' '+df3['Genre2']
##  list_titles = list_titles.tolist()
## u can try to join "Title" + "English" and see waht hjappens ((shingeki no kyojin attack on titan)) But the below approach is better

list_titles  =df3['Title'].tolist()

anime_name = input(' Enter the anime you like : ')

find_close_match = difflib.get_close_matches(anime_name, list_titles)

close_match = find_close_match[0]
print(close_match)

anime_ID = df3[df3.Title == close_match]['ID'].values[0]

similarity_score = list(enumerate(similarity[anime_ID]))
print(similarity_score)

sort_sim_animes = sorted(similarity_score, key = lambda x:x[1], reverse = True) 
print(sort_sim_animes)

print('Animes for you : \n')

i = 1

for anime in sort_sim_animes:
    index = anime[0]
    title_from_ID = df3[df3.ID==index]['Title'].values[0]
    if (i<22):
      print(i, '.',title_from_ID)
      i+=1

"""So depending on the AnimeDB the model improves. AnimeDB4 is the best one.

** Animes like Citrus with a Yuri genre give us similar titles. ** Same with police titles like Zankyou no terror which give us Babylon, Death Nothe, da shi jie, saibi, similar titles.

Also splitting the Genres and Themes columns and only keeping the first columns from each didn't change the model compared to keeping the Genres and Themes columns alltogether.

When using Studio sometimes the model throws titles with the same Studio which it is not a good thing, hence it is fine to not consider Studio as a feature.

**Now we make the model without STUDIO and TITLE**

OC the model improves and throws result according to Genre and Title.

**Now we make the model without STUDIO and TITLE & SOURCE**

Source didn't give any predictive power to the model.
"""

df3.Synopsis.isna().sum()

df3.loc[df3.Title == "mahou no angel sweet mint"].head(24)

#df3.loc[df3.Year == 1990 ]

"""### If you don't know the Original Title you can use this one instead, introducing the Eng title- Keep in mind that many Titles don't have an English title on the dataset (unknown value)"""

list_titles2 =df3['English'].tolist()

anime_name_eng = input(' Enter the anime you like : ')

find_close_match2 = difflib.get_close_matches(anime_name_eng, list_titles2)

close_match2 = find_close_match2[0]

anime_ID2 = df3[df3.English == close_match2]['ID'].values[0]

similarity_score2 = list(enumerate(similarity[anime_ID2]))

sort_sim_animes2 = sorted(similarity_score2, key = lambda x:x[1], reverse = True) 

print('Animes for you : \n')

i = 1

for animeEn in sort_sim_animes2:
    index = animeEn[0]
    title_english_from_ID = df3[df3.ID==index]['Title'].values[0]
    if (i<8):
      print(i, '.',title_english_from_ID)
      i+=1

"""You can say that the results are the same despite this one is in english. You can change ** title_english_from_ID = df3[df3.ID==index]['Title'].values[0] ** the 'Title' for 'English' in that line of code if you want the Titles of the results on english

After doing a lot of data engineering and creating many Datasets with different lenghts, it is safe to say that it didn't improve much the Recomendation System. Hence the best approach could be to cluster the animes first and then building recommendation systems for each cluster. 

But isn't that already a recommendation system? Well yes but to a larger scale.

We can start a large clustering by simply dividing the data by Rating (children, teens, mature) BUT still we would need more data quality on GENRES and THEMES in order to create models for each.

Right now the RecoSys, works O.K. by recommending animes by genre and theme with any datasets (animeDB4, DB3, DB2, DB) with a little improvement while working with DB4, due to the lack of Mature/Children content on it (like the first cluster I had in mind in the previous paragraph) and because I choose 1990's up, because those animes have more data on Genres and Themes.

Still... I can say that it is not really good at all, because sometimes it recommends Titles with not too much in common more than the genre and/or theme GIVEN by the DATASET (I say that because I've watched animes and I know how similar an anime is to another), which means that it really needs to add more genres and themes to each Title in order to IMPROVE the System. I knew the dataset had little quality when I saw it but I still tried to, because... is fun to create models, ngl.

I was expecting to Synopsis to help due to the similarities between synopsis of some animes but it didn't. Actually in some cases the model running with Synopsis as a feature throws animes that have nothing to do with the anime we introduced.

Last, randomizing Themes with Nan values proportionally by themes didn't skew our model, either with 10% 20% or 0.1%. The results are still being similar. Most likely because those animes were unpopular and/or their Genres are enoguh to find similarities.
"""

df3.shape

